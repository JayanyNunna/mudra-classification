# Kuchipudi Mudra Classification ğŸ™

Deep learning-based classification system for Indian classical dance hand gestures (mudras) from Kuchipudi tradition.

## ğŸ¯ Project Overview

This project implements multiple approaches for mudra recognition:
- **VGG16 + SVM**: Transfer learning with traditional classifier
- **Fine-tuned ConvNeXt**: Modern CNN architecture
- **Real-time Detection**: MediaPipe + trained models for live inference

## ğŸ“Š Dataset

- **Classes**: 28 Kuchipudi mudra gestures
- **Split**: 70% train / 10% validation / 20% test
- **Source**: Kuchipudi Mudra Dataset

## ğŸš€ Quick Start

### Installation
```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/mudra-classification.git
cd mudra-classification

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Training Pipeline
```bash
# 1. Split dataset
python scripts/split_data.py

# 2. Extract VGG16 features
python src/feature_extraction.py

# 3. Train SVM classifier
python src/train.py

# 4. Evaluate model
python src/evaluate.py
python src/confusion_matrix.py

# 5. Visualize features (optional)
python src/feature_visualization.py
```

### Real-time Inference
```bash
# Using webcam
python deployment/realtime_predict.py

# Using MediaPipe hand detection
python deployment/mediapipe_predict.py

# Single image prediction
python deployment/predict_mudra.py --image tests/test_image.jpg
```

## ğŸ“ Project Structure
```
mudra-classification/
â”œâ”€â”€ data/                  # Dataset and metadata
â”‚   â”œâ”€â”€ Kuchipudi-Mudra-Dataset-master.zip
â”‚   â””â”€â”€ class_names.txt
â”œâ”€â”€ scripts/              # Data preprocessing
â”‚   â”œâ”€â”€ split_data.py
â”‚   â””â”€â”€ generate_class_names.py
â”œâ”€â”€ src/                  # Core ML pipeline
â”‚   â”œâ”€â”€ feature_extraction.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ confusion_matrix.py
â”‚   â””â”€â”€ feature_visualization.py
â”œâ”€â”€ models/               # Trained models
â”‚   â”œâ”€â”€ svm_mudra_model.pkl
â”‚   â”œâ”€â”€ vgg16_feature_extractor.h5
â”‚   â””â”€â”€ label_map.pkl
â”œâ”€â”€ features/            # Extracted features (not in git)
â”œâ”€â”€ deployment/          # Inference scripts
â”‚   â”œâ”€â”€ predict_mudra.py
â”‚   â”œâ”€â”€ realtime_predict.py
â”‚   â””â”€â”€ mediapipe_predict.py
â”œâ”€â”€ tests/              # Test images
â”œâ”€â”€ results/            # Evaluation outputs
â””â”€â”€ config.py          # Configuration
```

## ğŸ—ï¸ Model Architectures

### VGG16 + SVM
- Feature extractor: VGG16 (ImageNet pretrained, frozen at block5_pool)
- Classifier: SVM with RBF kernel (C=10)
- Input: 128Ã—128 RGB images

### ConvNeXt
- Fine-tuned ConvNeXt model
- End-to-end training

## ğŸ“ˆ Results

- **Test Accuracy**: [Add after training]
- **Validation Accuracy**: [Add after training]

View confusion matrix: `results/confusion_matrix.png`

## ğŸ› ï¸ Technologies

- **Deep Learning**: TensorFlow/Keras, PyTorch
- **ML**: scikit-learn
- **Computer Vision**: OpenCV, MediaPipe
- **Visualization**: Matplotlib, Seaborn
